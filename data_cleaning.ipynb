{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNIZ0oiqVv8q"
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Data visualization, cleaning, division, and normalization.\n",
    "\n",
    "* Execute small adjustments/renaming of columns\n",
    "* Disease parameter: DMD/Cnt -> 1/0\n",
    "* Remove rows if:\n",
    "  * Sample.ID's value is \"BLANK\", \"POOL 1\" or \"POOL 2\"\n",
    "  * Disease or Sample.ID value for row is missing  \n",
    "* If there are multiple rows with same Sample.ID, drop the duplicate rows with fewer value entires\n",
    "  * Might need to evalueate manually or reevaluate the heuristiics (which proteins to prioritize over others)\n",
    "* Remove columns of antibody consentrations if there are no data entries\n",
    "\n",
    "* TODO: debugg, continue with later points\n",
    "* Potentionally normalize intensities: [-1, 1] or [0, 1]\n",
    "* Create new column for LoA based on FT1-5 \n",
    "\n",
    "* Flytta om ordningen på kolumnerna så att varje rad blir en vektor på formen <<METADATA>,<Y>,<X>> där <X> är [LoA], <Y> är intensiteterna (och eventuellt ålder senare) och <METADATA> är allt annat.\n",
    "\n",
    "* Run a SVM with our input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroment to read and process DMD data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeiNQxvnXphM"
   },
   "source": [
    "Imports used for data processing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhloDaQfbCZK"
   },
   "source": [
    "Load CSV file through pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.615678Z",
     "start_time": "2024-04-09T11:06:46.478256Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1710344581051,
     "user": {
      "displayName": "jnxmaster",
      "userId": "16130325099071186302"
     },
     "user_tz": -60
    },
    "id": "lUDzHuv3Xxcm",
    "outputId": "c9cdef14-583c-495f-e44c-a0ea72be5bdb"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('normalised_data_all_w_clinical_kex_20240321.csv')\n",
    "dataset.head() # Pre-view first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mU1AUw0annb"
   },
   "source": [
    "## First Data Visualization & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.648476Z",
     "start_time": "2024-04-09T11:06:46.616688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary for value conversion\n",
    "token_to_val = {\n",
    "    \"DMD\": 1,\n",
    "    \"Cnt\": 0\n",
    "}\n",
    "# Rename columns\n",
    "dataset.rename(columns={dataset.columns[0]: 'ID'}, inplace=True)\n",
    "dataset.rename(columns={'Sample.ID': 'Sample_ID'}, inplace=True)\n",
    "dataset.rename(columns={'Participant.ID': 'Participant_ID'}, inplace=True)\n",
    "dataset.rename(columns={'dataset': 'Dataset'}, inplace=True)\n",
    "dataset.rename(columns={'patregag': 'Age'}, inplace=True)\n",
    "\n",
    "# Replace the string values in the column using the mapping in token_to_val\n",
    "dataset['Disease'] = dataset['Disease'].replace(token_to_val)\n",
    "\n",
    "# Verify the change\n",
    "print(dataset.columns)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.659262Z",
     "start_time": "2024-04-09T11:06:46.650485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Give control group (non DMD) default value of 34 (top score) on FT5\n",
    "in_control = dataset['Disease']  == 0.0\n",
    "control_index = in_control[in_control == True].index\n",
    "dataset.loc[control_index, 'FT5'] = 34\n",
    "\n",
    "# Verify the change\n",
    "print(dataset.iloc[:15, 7:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Based Data Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.666097Z",
     "start_time": "2024-04-09T11:06:46.660268Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_column_value_percentage(df, start_column=1): # TODO: add end_column param\n",
    "    \"\"\"\n",
    "    Calculates the percentage of actual (non-NA) data points for each column in a pandas DataFrame\n",
    "    within a specified column interval.\n",
    "\n",
    "    :param df: A pandas DataFrame with potential NA values.\n",
    "    :param start_column: The starting column index for the interval (1-based index).\n",
    "    :param end_column: The ending column index for the interval. If None, calculates up to \n",
    "    the last column. \n",
    "    :return: A pandas Series with the percentage of non-NA values for each column in the interval.\n",
    "    \"\"\"\n",
    "    # Adjust for 0-based indexing\n",
    "    start_index = max(0, start_column - 1)\n",
    "\n",
    "    # Select only the columns within the specified interval\n",
    "    interval_df = df.iloc[:, start_index:]\n",
    "\n",
    "    # Calculate the total number of non-NA values for each column\n",
    "    value_counts = interval_df.count()\n",
    "\n",
    "    # Calculate the total number of rows (to handle potential NA rows)\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Calculate the percentage of non-NA values for each column\n",
    "    val_percentage = (value_counts / total_rows) * 100\n",
    "    \n",
    "    return val_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.678806Z",
     "start_time": "2024-04-09T11:06:46.667101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate column statistics for low content columns\n",
    "value_percentage = calculate_column_value_percentage(dataset, 15)\n",
    "limit = 50\n",
    "low_percentage_columns = value_percentage[value_percentage < limit]\n",
    "\n",
    "# Visualize status\n",
    "num = 0\n",
    "for column, percentage in low_percentage_columns.items():\n",
    "    print(f\"Column {column} has {percentage:.2f}% non-NA values\")\n",
    "    num += 1\n",
    "\n",
    "print(f\"We have {num} proteins with less than {limit}% datapoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.686507Z",
     "start_time": "2024-04-09T11:06:46.679813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove empty columns\n",
    "columns_to_drop = low_percentage_columns.index\n",
    "print(\"Columns to drop:\", columns_to_drop)\n",
    "\n",
    "# Check changes\n",
    "print(\"Before drop:\", dataset.shape)\n",
    "dataset.drop(labels=columns_to_drop, axis=\"columns\", inplace=True)\n",
    "print(\"After drop:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.696561Z",
     "start_time": "2024-04-09T11:06:46.687515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove abundant data and calibration columns\n",
    "print(\"Before drop:\", dataset.shape)\n",
    "dataset.drop(labels=['TREAT', 'Plate', 'Location', 'Empty_SBA1_rep1', 'Rabbit.IgG_SBA1_rep1'], axis='columns', inplace=True)\n",
    "print(\"After drop:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Based Data Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.702638Z",
     "start_time": "2024-04-09T11:06:46.697568Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_wrong_value_rows(df, column_name, wrong_val):\n",
    "    \"\"\"\n",
    "    Removes rows from the DataFrame where the specified column has the specified wrong value.\n",
    "\n",
    "    :param df: A pandas DataFrame from which rows will be removed.\n",
    "    :param column_name: The name of the column to check for the wrong value.\n",
    "    :param wrong_val: The value considered wrong in the specified column.\n",
    "    :return: A pandas DataFrame with rows containing the wrong value in the specified column removed.\n",
    "    \"\"\"\n",
    "    if isinstance(wrong_val, str):\n",
    "        wrong_val = list([wrong_val])\n",
    "        \n",
    "    for val in wrong_val:\n",
    "        # Find indices of rows with the wrong value\n",
    "        incorrect = dataset[column_name] == val\n",
    "        idxs_to_drop = incorrect[incorrect == True].index\n",
    "        # Drop these rows\n",
    "        df.drop(idxs_to_drop, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.722339Z",
     "start_time": "2024-04-09T11:06:46.705648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows with invalid sample data\n",
    "print(\"Before drop:\", dataset.shape)\n",
    "dataset = remove_wrong_value_rows(dataset, 'Sample_ID', ['BLANK', 'POOL 1', 'POOL 2'])\n",
    "print(\"After drop:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.732628Z",
     "start_time": "2024-04-09T11:06:46.723349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN in the row's key values\n",
    "print(\"Before drop:\", dataset.shape)\n",
    "dataset.dropna(subset=['Sample_ID','Disease'], inplace=True)\n",
    "print(\"After drop:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle sample duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.738776Z",
     "start_time": "2024-04-09T11:06:46.733636Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_duplicate_indices(df, cols):\n",
    "    \"\"\"\n",
    "    Find indices of rows with the wrong value in the specified column.\n",
    "    \"\"\"\n",
    "    duplicate = df.duplicated(subset=cols, keep=False)\n",
    "    duplicate_idxs = duplicate[duplicate == True].index\n",
    "    return duplicate_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.747653Z",
     "start_time": "2024-04-09T11:06:46.739782Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_row_value_percentage(df, start_column=0):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of actual (non-NA) data points for each row in a pandas DataFrame.\n",
    "\n",
    "    :param start_column: \n",
    "    :param df: A pandas DataFrame with potential NA values.\n",
    "    :return: A pandas Series with the percentage of non-NA values for each row.\n",
    "    \"\"\"\n",
    "    # Adjust for 0-based indexing\n",
    "    start_index = max(0, start_column - 1)\n",
    "\n",
    "    # Select only the columns within the specified interval\n",
    "    interval_df = df.iloc[:, start_index:]\n",
    "\n",
    "    # Calculate the number of non-NA values per row\n",
    "    value_counts_per_row = df.notna().sum(axis=1)\n",
    "\n",
    "    # Calculate the total number of columns (to handle potential NA values)\n",
    "    total_columns = interval_df.shape[1]\n",
    "\n",
    "    # Calculate the percentage of non-NA values for each row\n",
    "    value_percentage_per_row = (value_counts_per_row / total_columns) * 100\n",
    "\n",
    "    return value_percentage_per_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.757889Z",
     "start_time": "2024-04-09T11:06:46.748662Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_rows(df, duplicate_idxs, row_val_perc):\n",
    "\n",
    "    for i in duplicate_idxs:\n",
    "        # For each duplicate find the duplicate sample.ID value using the index\n",
    "        sample_ID = df.iloc[i]['Sample_ID']\n",
    "\n",
    "        # Find all row indices of occurrences of the value\n",
    "        duplicate_sample_ID_indices = df.index[df['Sample_ID'] == sample_ID]\n",
    "\n",
    "        # Find which of these rows have the highest percentage in row_val_percentages\n",
    "        best_index = -1\n",
    "        best_val = -1\n",
    "        for duplicate_idx in duplicate_sample_ID_indices:\n",
    "            val = row_val_perc.loc[duplicate_idx]\n",
    "\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_index = duplicate_idx\n",
    "\n",
    "        # Remove best from list of duplicates\n",
    "        duplicate_sample_ID_indices = duplicate_sample_ID_indices.drop(best_index)\n",
    "\n",
    "        # Drop the rest of the duplicates\n",
    "        df.drop(index=duplicate_sample_ID_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T11:06:46.851147Z",
     "start_time": "2024-04-09T11:06:46.759903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows for same Sample_ID\n",
    "duplicate_indexes = get_duplicate_indices(dataset, 'Sample_ID')\n",
    "row_val_percentages = calculate_row_value_percentage(dataset, start_column=15)\n",
    "\n",
    "# Check changes\n",
    "print(\"Before drop:\", dataset.shape)\n",
    "remove_duplicate_rows(dataset, duplicate_indexes, row_val_percentages)\n",
    "print(\"After drop:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row handling based on FT5 (Should consider data generation based on age/other tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the FT5 column\n",
    "not_na = dataset['FT5'].notna()\n",
    "indices_to_drop = not_na[not_na == False].index\n",
    "\n",
    "# Check changes\n",
    "print(\"Before drop:\", dataset.shape)\n",
    "dataset.drop(indices_to_drop, inplace=True)\n",
    "print(\"After drop:\", dataset.shape)\n",
    "\n",
    "dataset.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to a new CSV file\n",
    "dataset.to_csv('cleaned_data.csv', index=False)\n",
    "dataset.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
